{
  "license": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution blah blah blah, etc.",
  "modelfile": "# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this, replace FROM with:\n# FROM llava:latest\n\nFROM /Users/nick/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868\nFROM /Users/nick/.ollama/models/blobs/sha256-72d6f08a42f656d36b356dbe0920675899a99ce21192fd66266fb7d82ed07539\nTEMPLATE [INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST]\nPARAMETER stop [INST]\nPARAMETER stop [/INST]\nLICENSE \"\"\"                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\nyadda yadda.\"\"\"\n",
  "parameters": "stop                           \"[INST]\"\nstop                           \"[/INST]\"",
  "template": "[INST] {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }} [/INST]",
  "details": {
    "parent_model": "",
    "format": "gguf",
    "family": "llama",
    "families": ["llama", "clip"],
    "parameter_size": "7B",
    "quantization_level": "Q4_0"
  },
  "model_info": {
    "general.architecture": "llama",
    "general.file_type": 2,
    "general.parameter_count": 7241732096,
    "general.quantization_version": 2,
    "llama.attention.head_count": 32,
    "llama.attention.head_count_kv": 8,
    "llama.attention.layer_norm_rms_epsilon": 1.0e-5,
    "llama.block_count": 32,
    "llama.context_length": 32768,
    "llama.embedding_length": 4096,
    "llama.feed_forward_length": 14336,
    "llama.rope.dimension_count": 128,
    "llama.rope.freq_base": 1000000,
    "tokenizer.ggml.add_bos_token": true,
    "tokenizer.ggml.add_eos_token": false,
    "tokenizer.ggml.bos_token_id": 1,
    "tokenizer.ggml.eos_token_id": 2,
    "tokenizer.ggml.model": "llama",
    "tokenizer.ggml.padding_token_id": 0,
    "tokenizer.ggml.scores": null,
    "tokenizer.ggml.token_type": null,
    "tokenizer.ggml.tokens": null,
    "tokenizer.ggml.unknown_token_id": 0
  },
  "projector_info": {
    "clip.has_llava_projector": true,
    "clip.has_text_encoder": false,
    "clip.has_vision_encoder": true,
    "clip.projector_type": "mlp",
    "clip.use_gelu": false,
    "clip.vision.attention.head_count": 16,
    "clip.vision.attention.layer_norm_epsilon": 1.0e-5,
    "clip.vision.block_count": 23,
    "clip.vision.embedding_length": 1024,
    "clip.vision.feed_forward_length": 4096,
    "clip.vision.image_mean": [0.48145467, 0.4578275, 0.40821072],
    "clip.vision.image_size": 336,
    "clip.vision.image_std": [0.26862955, 0.2613026, 0.2757771],
    "clip.vision.patch_size": 14,
    "clip.vision.projection_dim": 768,
    "general.architecture": "clip",
    "general.description": "image encoder for LLaVA",
    "general.file_type": 1,
    "general.name": "openai/clip-vit-large-patch14-336",
    "general.parameter_count": 311888896
  },
  "modified_at": "2024-10-28T13:34:34.743965321-04:00"
}
